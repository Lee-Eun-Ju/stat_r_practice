library(dplyr)
library(corrplot)
library(psych)
library(ggplot2)
library(tidyr)

data=read.csv('C:/Users/samsung/Desktop/FIFA 2018 Statistics.csv',header = TRUE)
summary(data)
glimpse(data)

#변수 독립적으로 만들어주기
attach(data)

#numeric 아닌 변수들 제거
data=data[,c(4:20,22)]
glimpse(data)
dim(data)
#최종 : 관측치 128개 변수 18개

#NA를 0으로 바꿔주기
data[is.na(data)]=0
head(data)

#주성분분석(PCA;principal component analysis) 
#차원의 축소, 자료의 요약이 주목적이다.->자료한눈에 파악하기 쉬움 
#시각화를 하면 변수와 개체간의 연관성도 살펴볼 수 있다. 

df<-data[,-1] #예측하려는 것이 승패 1열 스토어는 제거
head(df)

evalue = eigen(df_corr)$values
evec = eigen(df_corr)$vectors

# Plot Eigenvalues / Represented Variance
eigenvalues <- data.frame(eigen(df_corr)$values)
colnames(eigenvalues) <- c("Values")
eigenvalues$Number <- 1:nrow(df_corr)
eigenvalues$RepresentedVariance <- NA
for (i in 1:nrow(df_corr)) {
  eigenvalues$RepresentedVariance[i] <- sum(evalue[1:i])/sum(evalue) * 
    100
}
eigenvalues$RepresentedVariance_text <- paste(round(eigenvalues$RepresentedVariance, 
                                                    0), " %")

e1 <- ggplot(eigenvalues, aes(Number, y = Values), group = 1)
e1 <- e1 + geom_bar(stat = "identity")
e1 <- e1 + geom_line(aes(y = Values), group = 2)
e1 <- e1 + xlab("Number [-]")
e1 <- e1 + ylab("Eigenvalue [-]")
e1 <- e1 + geom_hline(aes(yintercept = 1), col = "red")
e1 <- e1 + geom_text(aes(label = RepresentedVariance_text), nudge_y = 0.2)
e1 <- e1 + ggtitle("Eigenvalues and explained Variance")
e1 <- e1 + theme_bw()
e1 <- e1 + scale_x_continuous(breaks = seq(1, 10, 1))
e1


## PCA
gof = evalue/sum(evalue)*100
gof
round(gof,3)
sum(gof[1:7]) 

V = evec[,1:7]
V
rownames(V)=colnames(df)
round(V,3)
rowname = colnames(df)
rownames(V) = rowname
V
Z = scale(df)
PS = Z%*%V
head(PS)
dim(PS) #주성분 점수
PS=round(PS,3)

#새로운 변수 win 만들기
win=rep(0,128)
n = 2*(1:64)-1
for (i in n) {
  if (data[i,1]>data[i+1,1]){
    win[i] = "Winner"
    win[i+1] = "Loser"
  }
  else if (data[i,1] == data[i+1,1]){
    if(PSO[i]=='Yes'){
      if(Goals.in.PSO[i]>Goals.in.PSO[i+1]){
        win[i]="Winner"
        win[i+1] = "Loser"
      }
      else{
        win[i+1]="Winner"
        win[i] = "Loser"
      }
    }
  else{
      win[i]=NA
      win[i+1]=NA
  }
  }
  else{
    win[i] = "Loser"
    win[i+1] = "Winner"
  }
    }

win
PS=cbind(PS,win) #새로운 변수 win 합치기
PS=data.frame(PS)
head(PS)
for (i in 1:7) {
  PS[,i]=as.character(PS[,i])
  PS[,i]=as.numeric(PS[,i])
  
} #수치형으로 바꿔주기
PS=PS[-which(is.na(PS$win)),]
glimpse(PS)

set.seed(123)
n <- nrow(PS)
idx <- 1:n
train_idx <- sample(idx,n*.7)
training <- PS[train_idx,]
validation <- PS[-train_idx,]
nrow(training)
nrow(validation)

########랜덤포레스트#########
set.seed(123)
library(randomForest)

ad_rf=randomForest(win~.,training,n.trees=500)
ad_rf
plot(ad_rf)

y_obs=ifelse(validation$win=='Winner',1,0)
yhat_rf=predict(ad_rf,validation,type = 'prob')[,'Winner']
pred_rf=prediction(yhat_rf,y_obs)
perf_rf=performance(pred_rf,measure = 'tpr',x.measure = 'fpr')
x11()
plot(perf_rf,col='red',main='ROC Curve')
abline(0,1)
performance(pred_rf,'auc')@y.values[[1]]

######GLM#############
glimpse(PS)
PS["win"]
PS["win"]<-ifelse(PS["win"]=="Winner",1,0) #반응변수 0,1로 바꿔주기
dim(PS)


set.seed(123)
n <- nrow(PS)
idx <- 1:n
train_idx <- sample(idx,n*.7)
training <- PS[train_idx,]
validation <- PS[-train_idx,]
nrow(training)
nrow(validation)

ps_mod<-glm(win~.,data=training)
y_obs=ifelse(validation$win==1,1,0)
yhat_glm=predict(ps_mod,validation)
pred_rf=prediction(yhat_glm,y_obs)
perf_rf=performance(pred_glm,measure = 'tpr',x.measure = 'fpr')
plot(perf_rf,col='blue',add=TRUE)
performance(pred_glm,'auc')@y.values[[1]]

install.packages('gridExtra')
library(gridExtra)
###예측값과 실제값 분포알아보기###
########rf########
p1=ggplot(data.frame(y_obs,yhat_rf),aes(y_obs,yhat_rf,group=y_obs,fill=factor(y_obs)))+geom_boxplot()

p2=ggplot(data.frame(y_obs,yhat_rf),aes(yhat_rf,fill=factor(y_obs)))+geom_density(alpha=0.5)

grid.arrange(p1,p2,ncol=2)


########glm######
p3=ggplot(data.frame(y_obs,yhat_glm),aes(y_obs,yhat_glm,group=y_obs,fill=factor(y_obs)))+geom_boxplot()

p4=ggplot(data.frame(y_obs,yhat_glm),aes(yhat_glm,fill=factor(y_obs)))+geom_density(alpha=0.5)

grid.arrange(p3,p4,ncol=2)

#########원데이터랑 비교#########
data
glimpse(data)
data=data[,-1]
ori=cbind(data,win)
ori=ori[-which(is.na(ori$win)),]

set.seed(123)
n <- nrow(ori)
idx <- 1:n
train_idx <- sample(idx,n*.7)
training <- ori[train_idx,]
validation <- ori[-train_idx,]
nrow(training)
nrow(validation)

##rf##
ad_rf=randomForest(win~.,training,n.trees=500)
ad_rf
plot(ad_rf)
y_obs=ifelse(validation$win=='Winner',1,0)
yhat_rf=predict(ad_rf,validation,type = 'prob')[,'Winner']
pred_rf=prediction(yhat_rf,y_obs)
perf_rf=performance(pred_rf,measure = 'tpr',x.measure = 'fpr')
x11()
plot(perf_rf,col='red',main='ROC Curve')
abline(0,1)
performance(pred_rf,'auc')@y.values[[1]]

###glm##
glimpse(ori)
ori['win']
ori['win']<-ifelse(ori['win']=="Winner",1,0)
dim(ori)

set.seed(123)
n <- nrow(ori)
idx <- 1:n
train_idx <- sample(idx,n*.7)
training <- ori[train_idx,]
validation <- ori[-train_idx,]
nrow(training)
nrow(validation)

set.seed(123)
ps_mod<-glm(win~.,data=training)
y_obs=ifelse(validation$win==1,1,0)
yhat_glm=predict(ps_mod,validation)
pred_rf=prediction(yhat_glm,y_obs)
perf_rf=performance(pred_glm,measure = 'tpr',x.measure = 'fpr')
plot(perf_rf,col='red',add=TRUE)
abline(0,1)
performance(pred_glm,'auc')@y.values[[1]]
